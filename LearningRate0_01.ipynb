{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EyeDiseaseDetection.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "vZYRKX1eMrHc",
        "nIv-MOUc4qN0",
        "0ONqLEa07c4O",
        "IUFRsr1y72fc",
        "kZSAdFr_7_F4",
        "Eodg8nrd8KvR",
        "RWxhKovW8f8J",
        "YJDDpEdv_vIw",
        "MbFUdEN_8Ypv",
        "CURx1X198q0A"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/trushpatel97/ColabResearch/blob/master/LearningRate0_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRY9gfJjoMi4",
        "outputId": "b829e3e4-b850-46e6-c83a-37e36c122231",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xw_rFfNu3iUS"
      },
      "source": [
        "# **Eye Disease Detection**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0ks8VhK3092"
      },
      "source": [
        "**Steps taken to solve problem**\n",
        "* Step 1: Import common libraries\n",
        "* Step 2: Setup GPU\n",
        "* Step 3: Getting the data\n",
        "* Step 4: Build/Train a CNN\n",
        "* Step 5: Fit the data\n",
        "* Step 6: Improve the model\n",
        "* Step 7: Fit improved model\n",
        "* Step 8: Save the model\n",
        "* Step 9: Presenting results visually\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZYRKX1eMrHc"
      },
      "source": [
        "## **Global variables/Hyperparameters**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05pVTzhNMxBu"
      },
      "source": [
        "Image_height = 224\n",
        "Image_width = 224\n",
        "val_split = 0.20\n",
        "batches_size = 16\n",
        "lr = 0.01\n",
        "spe = 220\n",
        "vs = 32\n",
        "epoch = 6"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIv-MOUc4qN0"
      },
      "source": [
        "## **Step 1: Import common libraries**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gB-NfE9p3hWz"
      },
      "source": [
        "#Common machine learning libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "#Used for setting up GPU and image classifcation\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub \n",
        "\n",
        "#Used for seeing how many images we have within each folder\n",
        "import os\n",
        "\n",
        "#Used for split / viewing images later on\n",
        "import glob\n",
        "\n",
        "#Libraries needed for image classification\n",
        "from tensorflow import keras \n",
        "from keras.optimizers import Adam,Adamax\n",
        "from tensorflow.python.keras.layers import Input, Activation, Conv2D, MaxPool2D,MaxPooling2D, BatchNormalization, UpSampling2D, Lambda\n",
        "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential, Model\n",
        "from keras.applications import imagenet_utils\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ONqLEa07c4O"
      },
      "source": [
        "## **Step 2: Setup GPU**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4qobbQc5bvO",
        "outputId": "578b33b1-05c0-4ea7-ef6c-16955ceac30f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(\"GPU is working\" if tf.config.list_physical_devices(\"GPU\") else \" GPU is not working. Please change the runtime to use GPU\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is working\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUFRsr1y72fc"
      },
      "source": [
        "## **Step 3: Getting Data**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Mmb5rQU79IM"
      },
      "source": [
        "# Getting the file of the training set and testing set\n",
        "train_folder = \"/content/drive/My Drive/Research/train\"\n",
        "test_folder = \"/content/drive/My Drive/Research/test\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eWbEAsdE_j4H",
        "outputId": "840a920c-0de6-4464-a289-6c89b48937fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "#Code to iterate over all the files (Sub directories included). This will be used to see how many training and images we have\n",
        "CNV_train = os.listdir(\"/content/drive/My Drive/Research/train/CNV\")   \n",
        "DME_train = os.listdir(\"/content/drive/My Drive/Research/train/DME\")\n",
        "DRUSEN_train = os.listdir(\"/content/drive/My Drive/Research/train/DRUSEN\")\n",
        "NORMAL_train = os.listdir(\"/content/drive/My Drive/Research/train/NORMAL\")\n",
        "\n",
        "\n",
        "CNV_test = os.listdir(\"/content/drive/My Drive/Research/test/CNV\") \n",
        "DME_test = os.listdir(\"/content/drive/My Drive/Research/test/DME\")\n",
        "DRUSEN_test = os.listdir(\"/content/drive/My Drive/Research/test/DRUSEN\")\n",
        "NORMAL_test = os.listdir(\"/content/drive/My Drive/Research/test/NORMAL\")\n",
        "\n",
        "total_training = len(CNV_train + DME_train + DRUSEN_train + NORMAL_train)\n",
        "total_testing = len(CNV_test + DME_test + DRUSEN_test + NORMAL_test)\n",
        "\n",
        "print(\"We have\",total_training, \"total images in the training folder and\", total_testing, \"images in the testing folder\")\n",
        "print(f\"Allocating {(1-val_split)*100}% for training {val_split*100}% for validation on the training set we get:\")\n",
        "print(total_training*(1-val_split), \"images for training\")\n",
        "print(total_training*(val_split), \"images for validation\")"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 83484 total images in the training folder and 1000 images in the testing folder\n",
            "Allocating 80.0% for training 20.0% for validation on the training set we get:\n",
            "66787.2 images for training\n",
            "16696.8 images for validation\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQCGevjaGZQJ",
        "outputId": "08b5e0ef-3190-42cc-f428-a9f9275240d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "#Creating batches\n",
        "train_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input,validation_split=val_split) \\\n",
        "    .flow_from_directory(directory=train_folder, target_size=(Image_height,Image_width), classes=['CNV','DME','DRUSEN','NORMAL'], batch_size=batches_size,class_mode=\"categorical\",\n",
        "                              subset=\"training\")\n",
        "validation_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input,validation_split=val_split) \\\n",
        "    .flow_from_directory(directory=train_folder, target_size=(Image_height,Image_width), classes=['CNV','DME','DRUSEN','NORMAL'], batch_size=batches_size,class_mode=\"categorical\",\n",
        "                              subset=\"validation\")\n",
        "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input) \\\n",
        "                       .flow_from_directory(test_folder, target_size=(Image_height,Image_width), \n",
        "                         classes=['CNV','DME','DRUSEN','NORMAL'], batch_size=batches_size,class_mode=\"categorical\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 66788 images belonging to 4 classes.\n",
            "Found 16696 images belonging to 4 classes.\n",
            "Found 1000 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZSAdFr_7_F4"
      },
      "source": [
        "## **Step 4: Build/Train a CNN**\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buo55eOK8GHP"
      },
      "source": [
        "#building the model to train a CNN. This will be used to extract features of images\n",
        "model = Sequential([\n",
        "    Conv2D(filters=32, kernel_size=(3, 3), activation='relu', input_shape=(Image_height,Image_width,3)),\n",
        "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
        "    Conv2D(filters=64, kernel_size=(3, 3), activation='relu'),\n",
        "    MaxPool2D(pool_size=(2, 2), strides=2),\n",
        "    Flatten(),\n",
        "    Dense(units=4, activation='softmax')\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pe_ATN6HmKf4",
        "outputId": "547f9333-c8e7-477f-b8c4-8e72a3e9b587",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 353
        }
      },
      "source": [
        "#Checking the models summary\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d (Conv2D)              (None, 298, 194, 32)      896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D) (None, 149, 97, 32)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_1 (Conv2D)            (None, 147, 95, 64)       18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 73, 47, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 219584)            0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 4)                 878340    \n",
            "=================================================================\n",
            "Total params: 897,732\n",
            "Trainable params: 897,732\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eodg8nrd8KvR"
      },
      "source": [
        "## **Step 5: Fit the data**\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ClNcqC9G8N7i"
      },
      "source": [
        "#Compiling the model\n",
        "model.compile(optimizer=Adam(learning_rate=lr), loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHg3k3Xrm-Sp",
        "outputId": "479513c1-6aa3-4bd8-c73f-9fa875050f9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 370
        }
      },
      "source": [
        "#Fitting the data\n",
        "model.fit(train_batches, steps_per_epoch = spe, epochs = epoch, \n",
        "            validation_data = vs, validation_steps = vs,shuffle = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "128/128 [==============================] - 1893s 15s/step - loss: 6.1900 - accuracy: 0.4561 - val_loss: 3.1588 - val_accuracy: 0.4326\n",
            "Epoch 2/10\n",
            "128/128 [==============================] - 1604s 13s/step - loss: 1.9485 - accuracy: 0.5793 - val_loss: 1.3147 - val_accuracy: 0.6201\n",
            "Epoch 3/10\n",
            "128/128 [==============================] - 1464s 11s/step - loss: 1.0720 - accuracy: 0.6740 - val_loss: 1.1694 - val_accuracy: 0.6445\n",
            "Epoch 4/10\n",
            "128/128 [==============================] - 1444s 11s/step - loss: 0.9682 - accuracy: 0.6892 - val_loss: 0.9238 - val_accuracy: 0.6865\n",
            "Epoch 5/10\n",
            "128/128 [==============================] - 1289s 10s/step - loss: 0.7701 - accuracy: 0.7217 - val_loss: 0.8793 - val_accuracy: 0.7012\n",
            "Epoch 6/10\n",
            "128/128 [==============================] - 1331s 10s/step - loss: 0.6729 - accuracy: 0.7637 - val_loss: 0.7865 - val_accuracy: 0.7178\n",
            "Epoch 7/10\n",
            "128/128 [==============================] - 1155s 9s/step - loss: 0.6316 - accuracy: 0.7793 - val_loss: 0.7348 - val_accuracy: 0.7383\n",
            "Epoch 8/10\n",
            "128/128 [==============================] - 1089s 9s/step - loss: 0.5662 - accuracy: 0.7991 - val_loss: 0.6929 - val_accuracy: 0.7520\n",
            "Epoch 9/10\n",
            "128/128 [==============================] - 1030s 8s/step - loss: 0.5703 - accuracy: 0.8057 - val_loss: 0.7513 - val_accuracy: 0.7324\n",
            "Epoch 10/10\n",
            "128/128 [==============================] - 936s 7s/step - loss: 0.4924 - accuracy: 0.8274 - val_loss: 0.7007 - val_accuracy: 0.7451\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f3c380d6c50>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sKxmA6yQwenD"
      },
      "source": [
        "## Step 6: Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v22ahOd9wpwW",
        "outputId": "a56b85bc-11e8-4bd4-cafc-793ec8567650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        }
      },
      "source": [
        "model.save(\"CNN.h5\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-34c945ce2b75>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"CNN.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h_8H7cYfwxqJ"
      },
      "source": [
        "## Step 7: Improve the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjJABj3fuuBZ"
      },
      "source": [
        "### VGG16 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2k3zpaETvU5J"
      },
      "source": [
        "#Function to create model. We will be using a pretrained model\n",
        "def create():\n",
        "  vgg16_model = keras.applications.vgg16.VGG16(input_tensor=Input(shape=(Image_height, Image_width, 3)),input_shape=(Image_height,Image_width,3), include_top = False)\n",
        "  model = Sequential()\n",
        "  model.add(vgg16_model)\n",
        "  for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(4, activation='softmax'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1y4eMejNvJf0"
      },
      "source": [
        "model = model.create()\n",
        "model.compile(Adam(lr=0.0001),loss=\"categorical_crossentropy\",metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwHpSxmQvFUP"
      },
      "source": [
        "model.fit(train_batches, steps_per_epoch=spe,\n",
        "                    validation_data=validation_batches_10,validation_steps=vs, epochs=epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4j3k1Yaf8nCV"
      },
      "source": [
        "model.save(\"VGG16_model.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JnlB5DiDq3ge"
      },
      "source": [
        "### Xception Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3mqyjP1HQBW"
      },
      "source": [
        "def create():\n",
        "  xception_model = tf.keras.applications.Xception(input_tensor=Input(shape=(Image_height, Image_width, 3)),input_shape=(Image_height,Image_width,3), include_top = False)\n",
        "  model = Sequential()\n",
        "  model.add(xception_model)\n",
        "  for layer in model.layers:\n",
        "    layer.trainable = False\n",
        "  model.add(Flatten())\n",
        "  model.add(Dense(4, activation='softmax'))\n",
        "  return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzIAXDsLPCgV",
        "outputId": "b0f03f34-4860-4e65-ca3c-571466c6d543",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "model = create()\n",
        "model.compile(Adam(lr=lr),loss=\"categorical_crossentropy\",metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/xception/xception_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "83689472/83683744 [==============================] - 2s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_5_lqAdPHFC",
        "outputId": "f433426c-daab-47c1-afb1-ba4a33105da4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "xception (Functional)        (None, 6, 10, 2048)       20861480  \n",
            "_________________________________________________________________\n",
            "flatten_5 (Flatten)          (None, 122880)            0         \n",
            "_________________________________________________________________\n",
            "dense_16 (Dense)             (None, 4)                 491524    \n",
            "=================================================================\n",
            "Total params: 21,353,004\n",
            "Trainable params: 491,524\n",
            "Non-trainable params: 20,861,480\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_knHbi9PKgK",
        "outputId": "7c4e32b9-e3be-43f1-be57-6ff5ce213567",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        }
      },
      "source": [
        "model.fit(train_batches, steps_per_epoch=spe,\n",
        "                    validation_data=validation_batches,validation_steps=vs, epochs=epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "512/512 [==============================] - 5606s 11s/step - loss: 7.4320 - accuracy: 0.5426 - val_loss: 8.0523 - val_accuracy: 0.4859\n",
            "Epoch 2/10\n",
            "512/512 [==============================] - 5079s 10s/step - loss: 6.7518 - accuracy: 0.5625 - val_loss: 7.8662 - val_accuracy: 0.6047\n",
            "Epoch 3/10\n",
            "512/512 [==============================] - 4459s 9s/step - loss: 7.1621 - accuracy: 0.5709 - val_loss: 8.9813 - val_accuracy: 0.5594\n",
            "Epoch 4/10\n",
            "512/512 [==============================] - 4002s 8s/step - loss: 7.1249 - accuracy: 0.5852 - val_loss: 10.5663 - val_accuracy: 0.5512\n",
            "Epoch 5/10\n",
            "512/512 [==============================] - 3659s 7s/step - loss: 7.0733 - accuracy: 0.5764 - val_loss: 8.5909 - val_accuracy: 0.5840\n",
            "Epoch 6/10\n",
            "512/512 [==============================] - ETA: 0s - loss: 6.5823 - accuracy: 0.6037"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INGRIXs1vwZh"
      },
      "source": [
        "model.save(\"Xception.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqmxsIYPY90f"
      },
      "source": [
        "### MobileNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjQiv6GDZGD9"
      },
      "source": [
        "mobile = tf.keras.applications.mobilenet.MobileNet(include_top=False,\n",
        "                                                           input_shape=(224, 224,3),\n",
        "                                                           pooling='max', weights='imagenet',\n",
        "                                                           alpha=1, depth_multiplier=1,dropout=.5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CFZlf-JUORM"
      },
      "source": [
        "x=mobile.layers[-1].output\n",
        "x=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n",
        "predictions=Dense (4, activation='softmax')(x)\n",
        "model = Model(inputs=mobile.input, outputs=predictions)    \n",
        "for layer in model.layers:\n",
        "    layer.trainable=True\n",
        "model.compile(Adamax(lr=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "checkpoint=tf.keras.callbacks.ModelCheckpoint(filepath=\"/content/drive/My Drive/Research/ModelCheckpoint\", monitor='val_loss', verbose=0, save_best_only=True,\n",
        "    save_weights_only=False, mode='auto', save_freq='epoch', options=None)\n",
        "lr_adjust=tf.keras.callbacks.ReduceLROnPlateau( monitor=\"val_loss\", factor=0.5, patience=1, verbose=0, mode=\"auto\",\n",
        "    min_delta=0.00001,  cooldown=0,  min_lr=0) \n",
        "callbacks=[checkpoint, lr_adjust]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "db_G7RygV7Zy"
      },
      "source": [
        "model.fit(train_batches, steps_per_epoch=spe,\n",
        "                    validation_data=validation_batches,validation_steps=vs, epochs=epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PVsjpGRwwKCl"
      },
      "source": [
        "model.save(\"Mobilenet.h5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcBH4jxVX7mZ"
      },
      "source": [
        "## Step 8: Improve MobileNet Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LB4v99EuYSac"
      },
      "source": [
        "mobile = tf.keras.applications.mobilenet.MobileNet(include_top=False,\n",
        "                                                           input_shape=(224, 224,3),\n",
        "                                                           pooling='max', weights='imagenet',\n",
        "                                                           alpha=1, depth_multiplier=1,dropout=.5)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kdHXJYJ_kJfd"
      },
      "source": [
        "x=mobile.layers[-1].output\n",
        "x=keras.layers.BatchNormalization(axis=-1, momentum=0.99, epsilon=0.001 )(x)\n",
        "predictions=Dense (4, activation='softmax')(x)\n",
        "model = Model(inputs=mobile.input, outputs=predictions)    \n",
        "for layer in model.layers:\n",
        "    layer.trainable=True\n",
        "model.compile(Adamax(lr=lr), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "checkpoint=tf.keras.callbacks.ModelCheckpoint(filepath=\"/content/drive/My Drive/Research/ModelCheckpoint\", monitor='val_loss', verbose=0, save_best_only=True,\n",
        "    save_weights_only=False, mode='auto', save_freq='epoch', options=None)\n",
        "lr_adjust=tf.keras.callbacks.ReduceLROnPlateau( monitor=\"val_loss\", factor=0.5, patience=1, verbose=0, mode=\"auto\",\n",
        "    min_delta=0.00001,  cooldown=0,  min_lr=0) \n",
        "callbacks=[checkpoint, lr_adjust]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YHlNYe5ikNcy",
        "outputId": "2a15e547-acbb-4dd6-8560-9b50c98e3ccd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 258
        }
      },
      "source": [
        "model.fit(train_batches, steps_per_epoch=spe,\n",
        "                    validation_data=validation_batches,validation_steps=vs, epochs=epoch)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/6\n",
            "220/220 [==============================] - 1131s 5s/step - loss: 0.9101 - accuracy: 0.7449 - val_loss: 0.9086 - val_accuracy: 0.6211\n",
            "Epoch 2/6\n",
            "220/220 [==============================] - 1063s 5s/step - loss: 0.5694 - accuracy: 0.8372 - val_loss: 0.4709 - val_accuracy: 0.8730\n",
            "Epoch 3/6\n",
            "220/220 [==============================] - 1023s 5s/step - loss: 0.5333 - accuracy: 0.8577 - val_loss: 0.8581 - val_accuracy: 0.7656\n",
            "Epoch 4/6\n",
            "220/220 [==============================] - 961s 4s/step - loss: 0.4719 - accuracy: 0.8699 - val_loss: 0.5116 - val_accuracy: 0.8965\n",
            "Epoch 5/6\n",
            "220/220 [==============================] - 907s 4s/step - loss: 0.4555 - accuracy: 0.8699 - val_loss: 0.5886 - val_accuracy: 0.8125\n",
            "Epoch 6/6\n",
            "220/220 [==============================] - 856s 4s/step - loss: 0.4185 - accuracy: 0.8790 - val_loss: 0.7532 - val_accuracy: 0.8613\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f741e415390>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ax47fyT---7J"
      },
      "source": [
        "model.save(\"Mobilenet_v2_001.h5\")#(Size of batches 16)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CURx1X198q0A"
      },
      "source": [
        "## **Step 9: Testing results**\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7pACj5dL8u2P"
      },
      "source": [
        "#Load model if needed\n",
        "model = keras.models.load_model('/content/Mobilenet_v2_001.h5')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75AZoH3vEuIx"
      },
      "source": [
        "model.compile(optimizer = 'adamax', loss = 'categorical_crossentropy', metrics = ['accuracy'])"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3Ky7-NDuuet",
        "outputId": "0aed031c-0ee3-4132-b68c-a906607f6126",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "test_folder = \"/content/drive/My Drive/Research/test\"\n",
        "test_batches = ImageDataGenerator(preprocessing_function=tf.keras.applications.mobilenet.preprocess_input) \\\n",
        "                       .flow_from_directory(test_folder, target_size=(Image_height,Image_width), \n",
        "                         classes=['CNV','DME','DRUSEN','NORMAL'], batch_size=batches_size,class_mode=\"categorical\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1000 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T7p_AqDejH3"
      },
      "source": [
        "predictions = model.predict(x=test_batches,verbose=0)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T07IBcV6er04",
        "outputId": "47371c21-6b2c-4a5c-9d59-4abed0d6ad55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "np.round(predictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1.],\n",
              "       [0., 1., 0., 0.],\n",
              "       [1., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 1., 0.],\n",
              "       [1., 0., 0., 0.],\n",
              "       [0., 0., 0., 1.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "05nLs_TkA1Tj",
        "outputId": "088ee510-a02f-46b1-908f-bf069906f46f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "# Predict the accuracy on the Test set\n",
        "acc = model.evaluate_generator(test_batches, steps=len(test_batches), verbose=1)\n",
        "print(\"Model Accuracy on Test Data\", acc[1]*100)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From <ipython-input-19-195100b31623>:2: Model.evaluate_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use Model.evaluate, which supports generators.\n",
            "63/63 [==============================] - 4s 71ms/step - loss: 0.1766 - accuracy: 0.9500\n",
            "Model Accuracy on Test Data 94.9999988079071\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKzaOXMqu14y",
        "outputId": "c8bf8629-f1d4-41a3-fa09-fa15189081aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "cm = confusion_matrix(y_true=test_batches.classes, y_pred=np.argmax(predictions, axis=-1))\n",
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "\n",
        "    print(cm)\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "cm_plot_labels = ['CNV','DME','DRUSEN','NORMAL']\n",
        "plot_confusion_matrix(cm=cm, classes=cm_plot_labels, title='Confusion Matrix')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-4b37a322f948>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_batches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mplot_confusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Confusion matrix'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBlues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mcm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnewaxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Normalized confusion matrix\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36margmax\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36margmax\u001b[0;34m(a, axis, out)\u001b[0m\n\u001b[1;32m   1184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1185\u001b[0m     \"\"\"\n\u001b[0;32m-> 1186\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'argmax'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1187\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapfunc\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbound\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_wrapit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapit\u001b[0;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0mwrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/numpy/core/_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[0;32m---> 85\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m__array__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    846\u001b[0m         \u001b[0;34m\"Cannot convert a symbolic Tensor ({}) to a numpy array.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    847\u001b[0m         \u001b[0;34m\" This error may indicate that you're trying to pass a Tensor to\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m         \" a NumPy call, which is not supported\".format(self.name))\n\u001b[0m\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Cannot convert a symbolic Tensor (dense/Softmax:0) to a numpy array. This error may indicate that you're trying to pass a Tensor to a NumPy call, which is not supported"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B07RpEQdv-17"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}